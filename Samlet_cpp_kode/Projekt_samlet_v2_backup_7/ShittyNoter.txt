 - Under bestemmelse af feasibility vha. RMS-beregning for subsets af hele billedet
vælger jeg at starte med at lave et nyt array, som er et subset af hele billedet. Dette
er alternativet til blot at læse subsettet af hele billedet hver gang, da det måske er 
hurtigere at læse det nye subarray for sig selv, idet access hermed bliver sekventiel og
ikke opdelt, som det ville være i hele billedet. Altså: Vi laver en kopi af et område af
hele billedet fra kameraet i stedet for blot at kigge i det område i billedet fra kameraet
hver gang.

 - Hvis der vælges et grid, f.eks. 10x10, er der stor sandsynlighed for, at variansen i hvert område
 er høj nok, idet der er stor sandsynlighed for at "fange" en feature i hvert område. Et stort område
på eks. 40x40 pixels vil tage for lang tid at lave SAD på, idet det vokser med O(n^2) (right?).
Så en øvre grænse på eks. 8x8 pixels eller noget er måske bedst? En idé her ville være at lave et stort 
RMS-array vha. inddeling af billedet i områder á 8x8 pixels, udvælge de indices med høj-nok-varians,
og så bare smide random indicies af disse væk, indtil der kun er 100 brugbare features tilbage.

 - Med et webcambillede på 640x480 og en kernelopløsning på 8x8 pixels (giver 80x60 bins/rektangler/whatever),
tager det omkring 38 ms at processere et helt billede! Med abs(pixel-mean) tager det ca. 29 ms.
Det kunne godt være man skulle bruge noget helt simpelt sobel-agtigt af hensyn til hastighed.
Image_subset_copy tager ca. 39% af denne tid, mens RMS tager 61%.

- Ud fra vores thresholdede feature-billede laver vi en liste over "valide" indices, altså en liste over, på hvilke koordinater, der er gode features. Med denne liste udvælger vi tilfældige punkter, indtil vi har 100 features (eller hvor mange vi nu end vil tracke). Hvis der er færre end 100 features, udvælger vi blot alle features. Ved at have en liste over "hvor features er", behøver vi kun 100 terningekast, før vi har 100 tilfældigt placerede punkter - i stedet for at vi hver gang skal teste, om den tilfældige placering også ligger oven i en feature.

- Hvis der er færre end f.eks. 3 gode features, skal vi passe på med at bruge dem som gode data, og måske bruge information om historien af bevægelse og features lidt som predictor på, hvor vores features er nu - á la et Kalman filter.